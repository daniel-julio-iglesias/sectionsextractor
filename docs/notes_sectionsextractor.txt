# sectionsextractor


TODO: UNDER CONSTRUCTION!!!
TODO: If behind proxy, set your proxy inside config.py

==================================================

A kind of REAME.first file
... but in reality the content are my notes

==================================================

This is the very first version of a
Sections Extractor application
wrapped into a Web Framework.
Used for Recommendation Engine
Knowledge Base.

Materials were used from http://inventwithpython.com/


The application is web based using Flask.

You can run the application
(after installing it
as intended with the below section notes)

Linux
(venv) $ export FLASK_APP=sectionsextractor.py
MS
(venv) $ set FLASK_APP=sectionsextractor.py

(venv) $ flask run

..register your user...
..test introducing file name "01.txt" ...for Document Extractor
...test introducing url "https://en.wikibooks.org/wiki/GNU_Health/Families" ... for HTML Extractor

The below initial project notes are from my exercises based on
The Flask Mega-Tutorial
https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world

Enjoy it and please, let me know any comments to make it better/useful.
Thank you.


==================================================

TO DO: app sources download
$ git config --global http.proxy http://proxy.mycompany:80
$ git clone https://github.com/daniel-julio-iglesias/sectionsextractor
(...)

==================================================

Install these packages after app sources download


(venv) $ pip install flask
(venv) $ pip install --proxy http://user:pass@proxyAddress:proxyPort flask

(venv) $ pip install flask-wtf
(venv) $ pip install flask-sqlalchemy
(venv) $ pip install flask-migrate
(venv) $ pip install flask-login
(venv) $ pip install requests

===================================================

>>> import requests

>>> proxies = {
    'http': 'http://user:pass@proxyAddress:proxyPort',
    'https': 'http://user:pass@proxyAddress:proxyPort0',
}

>>> requests.get('http://example.org', proxies=proxies)

>>> res = requests.get('http://www.gutenberg.org/cache/epub/1112/pg1112.txt', proxies=proxies)
>>> type(res)
>>> res.status_code == requests.codes.ok
(int) 200

>>> len(res.text)
>>> print(res.text[:250])

>>> import requests, bs4
>>> res = requests.get('http://nostarch.com')
>>  res = requests.get('http://nostarch.com', proxies=proxies)
>>> res.raise_for_status()
>>> noStarchSoup = bs4.BeautifulSoup(res.text)
>>> type(noStarchSoup)

>>> import bs4, os
>>> basedir = os.getcwd()
>>> source_dir = os.path.join(basedir, 'app', 'static', 'app', 'app-kb', 'app-kb-source', '001')
>>> source_dir = source_dir + os.sep
>>> docFile = 'example.html'
>>> filename = source_dir + docFile
>>> ### exampleFile = open('example.html')
>>> exampleFile = open(filename)
>>> exampleSoup = bs4.BeautifulSoup(exampleFile.read())
>>> elems = exampleSoup.select('#author')
>>> type(elems)
<class 'list'>
>>> len(elems)
1
>>> type(elems[0])
<class 'bs4.element.Tag'>
>>> elems[0].getText()
'Al Sweigart'
>>> str(elems[0])
'<span id="author">Al Sweigart</span>'
>>> elems[0].attrs
{'id': 'author'}


>>> pElems = exampleSoup.select('p')
>>> str(pElems[0])
'<p>Download my <strong>Python</strong> book from <a href="http://
inventwithpython.com">my website</a>.</p>'
>>> pElems[0].getText()
'Download my Python book from my website.'
>>> str(pElems[1])
'<p class="slogan">Learn Python the easy way!</p>'
>>> pElems[1].getText()
'Learn Python the easy way!'
>>> str(pElems[2])
'<p>By <span id="author">Al Sweigart</span></p>'
>>> pElems[2].getText()
'By Al Sweigart



>>> import bs4
>>> ## soup = bs4.BeautifulSoup(open('example.html'))
>>> ### spanElem = soup.select('span')[0]
>>> spanElem = exampleSoup.select('span')[0]
>>> str(spanElem)
'<span id="author">Al Sweigart</span>'
>>> spanElem.get('id')
'author'
>>> spanElem.get('some_nonexistent_addr') == None
True
>>> spanElem.attrs
{'id': 'author'}

===================================================

GNU+Health%2FFamilies
GNUHealthFamilies.html
https://en.wikibooks.org/wiki/GNU_Health/Families



exampleSoup.select('p')
exampleSoup.select('h1')
exampleSoup.select('h2')

>>> import os, requests, bs4

"""
    sample use
    >>> from microblog import app
    >>> app.config['SECRET_KEY']
    'you-will-never-guess'
"""



>>> proxies = {
    'http': 'http://user:pass@proxyAddress:proxyPort',
    'https': 'http://user:pass@proxyAddress:proxyPort0',
}

>>> res = requests.get('https://en.wikibooks.org/wiki/GNU_Health/Families', proxies=proxies)

>>> res.status_code == requests.codes.ok


>>> gnuHealthSoup = bs4.BeautifulSoup(res.text)
>>> type(gnuHealthSoup)
<class 'bs4.BeautifulSoup'>
>>> isinstance(gnuHealthSoup, bs4.BeautifulSoup)
True

>>> pElems = gnuHealthSoup.select('p')
>>> str(pElems[0])
>>> pElems[0].getText()

>>> h1Elems = gnuHealthSoup.select('h1')
>>> str(h1Elems[0])
>>> h1Elems[0].getText()


>>> h2Elems = gnuHealthSoup.select('h2')
>>> str(h2Elems[0])
>>> h2Elems[0].getText()





===================================================


Apply the next db steps after downloading your app sources


Linux
(venv) $ export FLASK_APP=sectionsextractor.py
MS
(venv) $ set FLASK_APP=sectionsextractor.py

(venv) $ flask db upgrade


===================================================

TO DO: apply the next db steps after downloading your app sources

Linux
(venv) $ export FLASK_APP=qnarecom.py
MS
(venv) $ set FLASK_APP=qnarecom.py

(venv) $ flask db init
(venv) $ flask db migrate -m "users table"
(venv) $ flask db upgrade

===================================================

Run the application

Linux
(venv) $ export FLASK_APP=sectionsextractor.py
MS
(venv) $ set FLASK_APP=sectionsextractor.py

(venv) $ flask run


URL: http://localhost:5000/
URL: http://localhost:5000/index

===================================================


TODO: Make a form to introduce URL from where to extract sections
TODO: Implement the logic for HTML section text extraction process
TODO: Implememt functionality for text extraction from MS Word / or compatible document
TODO: Implememt functionality for text extraction from PDF document
TODO: Implememt functionality for text extraction from MS Excel / or compatible document


===================================================

